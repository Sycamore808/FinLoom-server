# Module 02 - ç‰¹å¾å·¥ç¨‹æ¨¡å—

## æ¦‚è¿°

ç‰¹å¾å·¥ç¨‹æ¨¡å—æ˜¯ FinLoom é‡åŒ–äº¤æ˜“ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œä¸“é—¨è´Ÿè´£ä»åŸå§‹é‡‘èæ•°æ®ä¸­æå–ã€æ„å»ºå’Œä¼˜åŒ–å„ç§æŠ•èµ„ç‰¹å¾ã€‚è¯¥æ¨¡å—ä¸ Module 01 (æ•°æ®ç®¡é“) ç´§å¯†é›†æˆï¼Œä¸ºåç»­çš„ AI æ¨¡å‹è®­ç»ƒã€å¸‚åœºåˆ†æå’ŒæŠ•èµ„å†³ç­–æä¾›é«˜è´¨é‡çš„ç‰¹å¾æ•°æ®ã€‚

## ä¸»è¦åŠŸèƒ½

### 1. æŠ€æœ¯æŒ‡æ ‡è®¡ç®— (Technical Indicators)
- **TechnicalIndicators**: è®¡ç®—å„ç§ç»å…¸æŠ€æœ¯åˆ†ææŒ‡æ ‡
- æ”¯æŒç§»åŠ¨å¹³å‡çº¿ã€RSIã€MACDã€å¸ƒæ—å¸¦ã€ATRã€éšæœºæŒ‡æ ‡ç­‰
- è‡ªåŠ¨åŒ–æ‰¹é‡æŒ‡æ ‡è®¡ç®—ï¼Œæ”¯æŒè‡ªå®šä¹‰å‚æ•°é…ç½®
- ä¸ Module 01 æ— ç¼é›†æˆï¼Œç›´æ¥å¤„ç†è‚¡ç¥¨ä»·æ ¼æ•°æ®

### 2. å› å­å‘ç°ä¸åˆ†æ (Factor Discovery)
- **FactorAnalyzer**: ä¼ ç»Ÿå› å­åˆ†æå·¥å…·ï¼Œè®¡ç®—ICã€IRã€æ’åºICç­‰æŒ‡æ ‡
- **NeuralFactorDiscovery**: åŸºäºæ·±åº¦å­¦ä¹ çš„æ™ºèƒ½å› å­å‘ç°å™¨
- è‡ªåŠ¨å‘ç°æŠ•èµ„å› å­ï¼Œè¯„ä¼°å› å­æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§
- æ”¯æŒå› å­ç»„åˆä¼˜åŒ–å’Œå¤šå‘¨æœŸå› å­åˆ†æ

### 3. æ—¶é—´åºåˆ—ç‰¹å¾ (Temporal Features)
- **TimeSeriesFeatures**: æ—¶é—´åºåˆ—ç‰¹å¾æå–å™¨
- åŠ¨é‡ç‰¹å¾ã€æ³¢åŠ¨ç‡ç‰¹å¾ã€è¶‹åŠ¿ç‰¹å¾çš„è‡ªåŠ¨æå–
- å¤šæ—¶é—´çª—å£ç‰¹å¾å·¥ç¨‹ï¼Œæ”¯æŒä¸åŒçš„æ—¶é—´å°ºåº¦åˆ†æ

### 4. å›¾ç‰¹å¾åˆ†æ (Graph Features)
- **GraphAnalyzer**: åŸºäºç›¸å…³æ€§çš„å›¾ç‰¹å¾åˆ†æ
- **GraphEmbeddingExtractor**: åŸºäºå›¾ç¥ç»ç½‘ç»œçš„é«˜çº§ç‰¹å¾æå–
- è‚¡ç¥¨å…³è”åˆ†æã€ç¤¾åŒºå‘ç°ã€ä¸­å¿ƒæ€§åˆ†æ
- æ”¯æŒå¤šç§å›¾æ„å»ºæ–¹æ³• (ç›¸å…³æ€§ã€åç›¸å…³ã€äº’ä¿¡æ¯)

### 5. æ•°æ®å­˜å‚¨ç®¡ç† (Storage Management)
- **FeatureDatabaseManager**: ä¸“ç”¨çš„ç‰¹å¾æ•°æ®åº“ç®¡ç†å™¨
- **FeatureCacheManager**: é«˜æ•ˆçš„å†…å­˜ç¼“å­˜ç³»ç»Ÿ
- æ”¯æŒç‰¹å¾æ•°æ®çš„æŒä¹…åŒ–å­˜å‚¨å’Œå¿«é€Ÿæ£€ç´¢

## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒé…ç½®

```python
# ç¡®ä¿å·²å®‰è£…ä¾èµ–åŒ…
import pandas as pd
import numpy as np
import torch  # ç”¨äºç¥ç»ç½‘ç»œç‰¹å¾å‘ç°

# å¯¼å…¥ Module 02 ç»„ä»¶
from module_02_feature_engineering import (
    TechnicalIndicators,
    FactorAnalyzer,
    NeuralFactorDiscovery,
    TimeSeriesFeatures,
    GraphAnalyzer,
    GraphEmbeddingExtractor,
    get_feature_database_manager
)

# å¯¼å…¥ Module 01 æ•°æ®ç®¡é“
from module_01_data_pipeline import AkshareDataCollector, get_database_manager
```

### åŸºç¡€ä½¿ç”¨ç¤ºä¾‹

```python
# 1. è·å–æ•°æ® (æ¥è‡ª Module 01)
collector = AkshareDataCollector(rate_limit=0.5)
symbols = ["000001", "600036", "000858"]  # å¹³å®‰é“¶è¡Œã€æ‹›å•†é“¶è¡Œã€äº”ç²®æ¶²

# è·å–å†å²æ•°æ®
stock_data = {}
for symbol in symbols:
    data = collector.fetch_stock_history(symbol, "20230101", "20241201")
    if not data.empty:
        stock_data[symbol] = data

print(f"åŠ è½½äº† {len(stock_data)} åªè‚¡ç¥¨çš„æ•°æ®")

# 2. è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
tech_calc = TechnicalIndicators()

for symbol, data in stock_data.items():
    # è®¡ç®—å…¨éƒ¨æŠ€æœ¯æŒ‡æ ‡
    indicators = tech_calc.calculate_all_indicators(data)
    print(f"{symbol} æŠ€æœ¯æŒ‡æ ‡: {indicators.columns.tolist()}")
    
    # ä¿å­˜åˆ°ç‰¹å¾æ•°æ®åº“
    feature_db = get_feature_database_manager()
    feature_db.save_technical_indicators(symbol, indicators)
    print(f"âœ“ {symbol} æŠ€æœ¯æŒ‡æ ‡å·²ä¿å­˜")

# 3. æ—¶é—´åºåˆ—ç‰¹å¾æå–
ts_extractor = TimeSeriesFeatures()

for symbol, data in stock_data.items():
    # æå–æ‰€æœ‰æ—¶é—´åºåˆ—ç‰¹å¾
    ts_features = ts_extractor.extract_all_features(data['close'])
    
    # ä¿å­˜æ—¶é—´åºåˆ—ç‰¹å¾
    feature_db.save_time_series_features(symbol, ts_features)
    print(f"âœ“ {symbol} æ—¶é—´åºåˆ—ç‰¹å¾å·²ä¿å­˜: {len(ts_features)} ä¸ªç‰¹å¾")

# 4. å› å­åˆ†æ
factor_analyzer = FactorAnalyzer()

# ä½¿ç”¨æŠ€æœ¯æŒ‡æ ‡ä½œä¸ºå› å­è¿›è¡Œåˆ†æ
for symbol, data in stock_data.items():
    # è®¡ç®—æ”¶ç›Šç‡
    returns = data['close'].pct_change().dropna()
    
    # ä½¿ç”¨RSIä½œä¸ºå› å­è¿›è¡Œåˆ†æ
    rsi = tech_calc.calculate_rsi(data['close'])
    factor_result = factor_analyzer.analyze_factor(rsi, returns)
    
    print(f"{symbol} RSIå› å­åˆ†æ:")
    print(f"  IC: {factor_result.ic:.4f}")
    print(f"  Rank IC: {factor_result.rank_ic:.4f}")
    print(f"  IR: {factor_result.ir:.4f}")
```

### ç¥ç»å› å­å‘ç°ç¤ºä¾‹

```
# 5. ç¥ç»å› å­å‘ç° (é«˜çº§åŠŸèƒ½)
from module_02_feature_engineering.factor_discovery.neural_factor_discovery import FactorConfig

# é…ç½®ç¥ç»å› å­å‘ç°å™¨
config = FactorConfig(
    input_dim=10,  # è¾“å…¥ç‰¹å¾ç»´åº¦
    hidden_dims=[64, 32, 16],
    output_dim=1,
    epochs=50,
    learning_rate=0.001
)

neural_discoverer = NeuralFactorDiscovery(config)

# ä» Module 01 åŠ è½½ç‰¹å¾æ•°æ®
features_df = neural_discoverer.load_features_from_module01(
    symbols=symbols,
    start_date="20230101", 
    end_date="20241201"
)

if not features_df.empty:
    # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
    feature_columns = ['returns', 'log_returns', 'volatility', 'volume_ratio']
    X = features_df[feature_columns].dropna()
    y = X['returns'].shift(-1).dropna()  # ä¸‹ä¸€æœŸæ”¶ç›Šä½œä¸ºç›®æ ‡
    
    # å¯¹é½æ•°æ®
    common_index = X.index.intersection(y.index)
    X_aligned = X.loc[common_index]
    y_aligned = y.loc[common_index]
    
    # å‘ç°ç¥ç»å› å­
    discovered_factors = neural_discoverer.discover_neural_factors(
        features=X_aligned, 
        returns=y_aligned
    )
    
    print(f"å‘ç°äº† {len(discovered_factors)} ä¸ªç¥ç»å› å­:")
    for factor in discovered_factors:
        print(f"  {factor.name}: IC={factor.ic_score:.4f}, é‡è¦æ€§={factor.importance_score:.4f}")
    
    # ä¿å­˜ç¥ç»å› å­åˆ°æ•°æ®åº“
    neural_discoverer.save_discovered_factors(discovered_factors)
    print("âœ“ ç¥ç»å› å­å·²ä¿å­˜åˆ°æ•°æ®åº“")
```

### å›¾ç‰¹å¾åˆ†æç¤ºä¾‹

```
# 6. å›¾ç‰¹å¾åˆ†æ
# æ„å»ºè‚¡ç¥¨æ”¶ç›Šç‡çŸ©é˜µ
returns_matrix = pd.DataFrame()
for symbol, data in stock_data.items():
    returns_matrix[symbol] = data['close'].pct_change()

returns_matrix = returns_matrix.dropna()

# åŸºç¡€å›¾åˆ†æ
graph_analyzer = GraphAnalyzer()
graph_features = graph_analyzer.extract_graph_features(returns_matrix)

print(f"å›¾ç‰¹å¾åˆ†æç»“æœ: {len(graph_features)} ä¸ªç‰¹å¾")
for feature_name, feature_obj in graph_features.items():
    print(f"  {feature_name}: {feature_obj.description}")

# é«˜çº§å›¾åµŒå…¥åˆ†æ (å¯é€‰ï¼Œéœ€è¦æ›´å¤šè®¡ç®—èµ„æº)
try:
    from module_02_feature_engineering.graph_features.graph_embeddings import GraphConfig
    
    # é…ç½®å›¾åµŒå…¥
    graph_config = GraphConfig(
        embedding_dim=32,
        hidden_dims=[64, 32],
        gnn_type="GAT",  # å›¾æ³¨æ„åŠ›ç½‘ç»œ
        epochs=50
    )
    
    graph_extractor = GraphEmbeddingExtractor(graph_config)
    
    # æ„å»ºè‚¡ç¥¨å…³è”å›¾
    stock_graph = graph_extractor.build_stock_correlation_graph(
        returns_matrix, 
        threshold=0.3,
        method="correlation"
    )
    
    print(f"è‚¡ç¥¨å…³è”å›¾: {stock_graph.number_of_nodes()} ä¸ªèŠ‚ç‚¹, {stock_graph.number_of_edges()} æ¡è¾¹")
    
    # è®¡ç®—ä¸­å¿ƒæ€§æŒ‡æ ‡
    centrality_df = graph_extractor.calculate_centrality_measures()
    print("è‚¡ç¥¨ä¸­å¿ƒæ€§æ’å (åº¦ä¸­å¿ƒæ€§):")
    top_central = centrality_df.sort_values('degree_centrality', ascending=False).head(3)
    for symbol, row in top_central.iterrows():
        print(f"  {symbol}: {row['degree_centrality']:.4f}")
    
    # æå–èŠ‚ç‚¹ç‰¹å¾
    node_features = returns_matrix.describe().T  # ä½¿ç”¨ç»Ÿè®¡ç‰¹å¾ä½œä¸ºèŠ‚ç‚¹ç‰¹å¾
    
    # æå–å›¾åµŒå…¥
    graph_embedding = graph_extractor.extract_graph_embeddings(node_features)
    
    print(f"å›¾åµŒå…¥ç»´åº¦: {graph_embedding.node_embeddings.shape}")
    print(f"å›¾çº§åˆ«ç‰¹å¾: {list(graph_embedding.graph_features.keys())}")
    
    # ä¿å­˜å›¾åµŒå…¥åˆ°æ•°æ®åº“
    for symbol in symbols:
        if symbol in graph_extractor.node_mapping:
            idx = graph_extractor.node_mapping[symbol]
            embedding = graph_embedding.node_embeddings[idx]
            feature_db.save_graph_embeddings(symbol, embedding, graph_config.__dict__)
    
    print("âœ“ å›¾åµŒå…¥ç‰¹å¾å·²ä¿å­˜")
    
except ImportError:
    print("âš  å›¾åµŒå…¥åŠŸèƒ½éœ€è¦é¢å¤–çš„æ·±åº¦å­¦ä¹ åº“ (torch_geometric)")
except Exception as e:
    print(f"âš  å›¾åµŒå…¥åˆ†æå‡ºé”™: {e}")
```

### ç‰¹å¾æ•°æ®æŸ¥è¯¢ç¤ºä¾‹

```
# 7. ç‰¹å¾æ•°æ®æŸ¥è¯¢å’Œç»Ÿè®¡
feature_db = get_feature_database_manager()

# æŸ¥è¯¢æŠ€æœ¯æŒ‡æ ‡
for symbol in symbols:
    indicators = feature_db.get_technical_indicators(symbol, "20241101", "20241201")
    if not indicators.empty:
        print(f"\n{symbol} æœ€æ–°æŠ€æœ¯æŒ‡æ ‡:")
        latest = indicators.iloc[-1]
        print(f"  SMA20: {latest.get('sma_20', 'N/A'):.2f}")
        print(f"  RSI: {latest.get('rsi', 'N/A'):.2f}")
        print(f"  MACD: {latest.get('macd', 'N/A'):.4f}")

# æŸ¥è¯¢æ—¶é—´åºåˆ—ç‰¹å¾
for symbol in symbols:
    ts_features = feature_db.get_time_series_features(symbol, "20241101", "20241201")
    if not ts_features.empty:
        print(f"\n{symbol} æ—¶é—´åºåˆ—ç‰¹å¾æ•°é‡: {ts_features.shape[1]}")

# æŸ¥è¯¢ç¥ç»å› å­
neural_factors = feature_db.get_neural_factors()
print(f"\næ•°æ®åº“ä¸­çš„ç¥ç»å› å­: {len(neural_factors)} ä¸ª")
for factor in neural_factors[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
    print(f"  {factor['factor_name']}: IC={factor['ic_score']:.4f}")

# æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
stats = feature_db.get_database_stats()
print(f"\nç‰¹å¾æ•°æ®åº“ç»Ÿè®¡:")
print(f"  æŠ€æœ¯æŒ‡æ ‡æ•°é‡: {stats.get('technical_indicators_count', 0):,}")
print(f"  å› å­æ•°æ®æ•°é‡: {stats.get('factor_data_count', 0):,}")
print(f"  ç¥ç»å› å­æ•°é‡: {stats.get('neural_factors_count', 0):,}")
print(f"  å›¾ç‰¹å¾æ•°é‡: {stats.get('graph_features_count', 0):,}")
print(f"  æ—¶é—´åºåˆ—ç‰¹å¾æ•°é‡: {stats.get('time_series_features_count', 0):,}")
print(f"  æ•°æ®åº“å¤§å°: {stats.get('database_size_mb', 0):.2f} MB")
print(f"  æ¶‰åŠè‚¡ç¥¨æ•°é‡: {stats.get('unique_symbols', 0)} åª")
```

## API å‚è€ƒ

### TechnicalIndicators

æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å™¨ï¼Œæä¾›å„ç§ç»å…¸æŠ€æœ¯åˆ†ææŒ‡æ ‡çš„è®¡ç®—åŠŸèƒ½ã€‚

#### æ„é€ å‡½æ•°
```
TechnicalIndicators()
```

#### ä¸»è¦æ–¹æ³•

**calculate_sma(data: pd.Series, period: int) -> pd.Series**
- è®¡ç®—ç®€å•ç§»åŠ¨å¹³å‡çº¿
- å‚æ•°ï¼šæ•°æ®åºåˆ—ã€å‘¨æœŸ

**calculate_ema(data: pd.Series, period: int) -> pd.Series**
- è®¡ç®—æŒ‡æ•°ç§»åŠ¨å¹³å‡çº¿
- å‚æ•°ï¼šæ•°æ®åºåˆ—ã€å‘¨æœŸ

**calculate_rsi(data: pd.Series, period: int = 14) -> pd.Series**
- è®¡ç®—ç›¸å¯¹å¼ºå¼±æŒ‡æ•°
- å‚æ•°ï¼šæ•°æ®åºåˆ—ã€å‘¨æœŸï¼ˆé»˜è®¤14ï¼‰

**calculate_macd(data: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9) -> Dict[str, pd.Series]**
- è®¡ç®—MACDæŒ‡æ ‡
- è¿”å›ï¼š{'macd': MACDçº¿, 'signal': ä¿¡å·çº¿, 'histogram': æŸ±çŠ¶å›¾}

**calculate_bollinger_bands(data: pd.Series, period: int = 20, std_dev: float = 2.0) -> Dict[str, pd.Series]**
- è®¡ç®—å¸ƒæ—å¸¦
- è¿”å›ï¼š{'upper': ä¸Šè½¨, 'middle': ä¸­è½¨, 'lower': ä¸‹è½¨}

**calculate_all_indicators(ohlcv_data: pd.DataFrame) -> pd.DataFrame**
- ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰æŠ€æœ¯æŒ‡æ ‡
- è¾“å…¥ï¼šOHLCVæ ¼å¼çš„DataFrame
- è¾“å‡ºï¼šåŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„DataFrame

#### ä½¿ç”¨ç¤ºä¾‹
```
calculator = TechnicalIndicators()

# å•ä¸ªæŒ‡æ ‡è®¡ç®—
sma20 = calculator.calculate_sma(stock_data['close'], 20)
rsi = calculator.calculate_rsi(stock_data['close'])
macd_data = calculator.calculate_macd(stock_data['close'])

# æ‰¹é‡è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
all_indicators = calculator.calculate_all_indicators(stock_data)
print(f"è®¡ç®—äº† {len(all_indicators.columns)} ä¸ªæŠ€æœ¯æŒ‡æ ‡")
```

### FactorAnalyzer

ä¼ ç»Ÿå› å­åˆ†æå·¥å…·ï¼Œç”¨äºè¯„ä¼°å› å­çš„æœ‰æ•ˆæ€§ã€‚

#### æ„é€ å‡½æ•°
```
FactorAnalyzer()
```

#### ä¸»è¦æ–¹æ³•

**calculate_ic(factor_values: pd.Series, returns: pd.Series) -> float**
- è®¡ç®—ä¿¡æ¯ç³»æ•° (Information Coefficient)
- è¡¡é‡å› å­é¢„æµ‹èƒ½åŠ›

**calculate_rank_ic(factor_values: pd.Series, returns: pd.Series) -> float**
- è®¡ç®—æ’åºä¿¡æ¯ç³»æ•° (Rank IC)
- åŸºäºæ’åºçš„ç›¸å…³æ€§åˆ†æ

**analyze_factor(factor_values: pd.Series, returns: pd.Series) -> FactorResult**
- ç»¼åˆå› å­åˆ†æ
- è¿”å›åŒ…å«ICã€IRã€æ¢æ‰‹ç‡ç­‰æŒ‡æ ‡çš„ç»“æœ

#### ä½¿ç”¨ç¤ºä¾‹
```
analyzer = FactorAnalyzer()

# ä½¿ç”¨æŠ€æœ¯æŒ‡æ ‡ä½œä¸ºå› å­
factor_values = indicators['rsi']
returns = stock_data['close'].pct_change()

# åˆ†æå› å­æœ‰æ•ˆæ€§
result = analyzer.analyze_factor(factor_values, returns)
print(f"IC: {result.ic:.4f}, Rank IC: {result.rank_ic:.4f}")
```

### NeuralFactorDiscovery

åŸºäºæ·±åº¦å­¦ä¹ çš„æ™ºèƒ½å› å­å‘ç°å™¨ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å‘ç°å’Œåˆ›å»ºæŠ•èµ„å› å­ã€‚

#### æ„é€ å‡½æ•°
```
NeuralFactorDiscovery(config: FactorConfig)
```

#### é…ç½®å‚æ•° (FactorConfig)
```
@dataclass
class FactorConfig:
    input_dim: int              # è¾“å…¥ç‰¹å¾ç»´åº¦
    hidden_dims: List[int]      # éšè—å±‚ç»´åº¦åˆ—è¡¨
    output_dim: int             # è¾“å‡ºç»´åº¦
    dropout_rate: float = 0.3   # Dropoutæ¯”ç‡
    learning_rate: float = 0.001 # å­¦ä¹ ç‡
    epochs: int = 100           # è®­ç»ƒè½®æ•°
    use_attention: bool = True  # æ˜¯å¦ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶
```

#### ä¸»è¦æ–¹æ³•

**load_features_from_module01(symbols: List[str], start_date: str, end_date: str) -> pd.DataFrame**
- ä»Module01åŠ è½½ç‰¹å¾æ•°æ®
- è‡ªåŠ¨è®¡ç®—åŸºç¡€ç‰¹å¾ï¼ˆæ”¶ç›Šç‡ã€æ³¢åŠ¨ç‡ç­‰ï¼‰

**discover_neural_factors(features: pd.DataFrame, returns: pd.Series) -> List[DiscoveredFactor]**
- å‘ç°ç¥ç»å› å­
- è¿”å›å‘ç°çš„å› å­åˆ—è¡¨

**save_discovered_factors(factors: List[DiscoveredFactor]) -> bool**
- ä¿å­˜å‘ç°çš„å› å­åˆ°æ•°æ®åº“

**extract_attention_features(features: pd.DataFrame) -> pd.DataFrame**
- æå–æ³¨æ„åŠ›åŠ æƒç‰¹å¾
- éœ€è¦æ¨¡å‹é…ç½®äº†æ³¨æ„åŠ›æœºåˆ¶

**evaluate_factor_effectiveness(factor_values: pd.Series, forward_returns: pd.Series) -> Dict[str, float]**
- è¯„ä¼°å› å­æœ‰æ•ˆæ€§
- è¿”å›å¤šå‘¨æœŸICã€IRç­‰æŒ‡æ ‡

#### ä½¿ç”¨ç¤ºä¾‹
```
# é…ç½®ç¥ç»å› å­å‘ç°å™¨
config = FactorConfig(
    input_dim=5,
    hidden_dims=[32, 16],
    output_dim=1,
    epochs=50
)

discoverer = NeuralFactorDiscovery(config)

# åŠ è½½æ•°æ®
features = discoverer.load_features_from_module01(
    symbols=["000001", "600036"], 
    start_date="20230101", 
    end_date="20241201"
)

# å‘ç°å› å­
factors = discoverer.discover_neural_factors(features, returns)

# ä¿å­˜ç»“æœ
discoverer.save_discovered_factors(factors)
```

### TimeSeriesFeatures

æ—¶é—´åºåˆ—ç‰¹å¾æå–å™¨ï¼Œä¸“é—¨å¤„ç†æ—¶é—´ç›¸å…³çš„ç‰¹å¾å·¥ç¨‹ã€‚

#### æ„é€ å‡½æ•°
```
TimeSeriesFeatures()
```

#### ä¸»è¦æ–¹æ³•

**extract_momentum_features(data: pd.Series, windows: List[int] = [5, 10, 20]) -> Dict[str, TimeSeriesFeature]**
- æå–åŠ¨é‡ç‰¹å¾
- åŒ…æ‹¬æ”¶ç›Šç‡ã€åŠ¨é‡æŒ‡æ ‡

**extract_volatility_features(data: pd.Series, windows: List[int] = [5, 10, 20]) -> Dict[str, TimeSeriesFeature]**
- æå–æ³¢åŠ¨ç‡ç‰¹å¾
- åŒ…æ‹¬æ»šåŠ¨æ ‡å‡†å·®ã€å˜å¼‚ç³»æ•°

**extract_trend_features(data: pd.Series, windows: List[int] = [5, 10, 20]) -> Dict[str, TimeSeriesFeature]**
- æå–è¶‹åŠ¿ç‰¹å¾
- åŒ…æ‹¬çº¿æ€§è¶‹åŠ¿æ–œç‡ã€è¶‹åŠ¿å¼ºåº¦

**extract_all_features(data: pd.Series) -> Dict[str, TimeSeriesFeature]**
- æå–æ‰€æœ‰æ—¶é—´åºåˆ—ç‰¹å¾

#### ä½¿ç”¨ç¤ºä¾‹
```
ts_extractor = TimeSeriesFeatures()

# æå–åŠ¨é‡ç‰¹å¾
momentum_features = ts_extractor.extract_momentum_features(
    stock_data['close'], 
    windows=[5, 10, 20, 30]
)

# æå–æ‰€æœ‰ç‰¹å¾
all_ts_features = ts_extractor.extract_all_features(stock_data['close'])
```

### GraphAnalyzer

å›¾ç‰¹å¾åˆ†æå™¨ï¼ŒåŸºäºè‚¡ç¥¨å…³è”å…³ç³»æ„å»ºå›¾å¹¶æå–å›¾ç‰¹å¾ã€‚

#### æ„é€ å‡½æ•°
```
GraphAnalyzer()
```

#### ä¸»è¦æ–¹æ³•

**build_correlation_graph(returns_matrix: pd.DataFrame, threshold: float = 0.3) -> Dict[str, List[str]]**
- æ„å»ºç›¸å…³æ€§å›¾
- è¿”å›å›¾çš„é‚»æ¥è¡¨è¡¨ç¤º

**calculate_centrality_measures(graph: Dict[str, List[str]]) -> Dict[str, Dict[str, float]]**
- è®¡ç®—ä¸­å¿ƒæ€§æŒ‡æ ‡
- åŒ…æ‹¬åº¦ä¸­å¿ƒæ€§ã€æ¥è¿‘ä¸­å¿ƒæ€§ã€ä»‹æ•°ä¸­å¿ƒæ€§

**extract_graph_features(returns_matrix: pd.DataFrame) -> Dict[str, GraphFeature]**
- æå–å›¾ç‰¹å¾

#### ä½¿ç”¨ç¤ºä¾‹
```
analyzer = GraphAnalyzer()

# æ„å»ºè‚¡ç¥¨å…³è”å›¾
returns_matrix = pd.DataFrame({
    'stock1': returns1,
    'stock2': returns2,
    'stock3': returns3
})

graph_features = analyzer.extract_graph_features(returns_matrix)
```

### GraphEmbeddingExtractor

åŸºäºå›¾ç¥ç»ç½‘ç»œçš„é«˜çº§å›¾ç‰¹å¾æå–å™¨ï¼Œèƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„è‚¡ç¥¨å…³è”æ¨¡å¼ã€‚

#### æ„é€ å‡½æ•°
```
GraphEmbeddingExtractor(config: Optional[GraphConfig] = None)
```

#### é…ç½®å‚æ•° (GraphConfig)
```
@dataclass
class GraphConfig:
    embedding_dim: int = 64      # åµŒå…¥ç»´åº¦
    hidden_dims: List[int] = [128, 64]  # éšè—å±‚ç»´åº¦
    gnn_type: str = "GAT"        # GNNç±»å‹: 'GCN', 'GAT', 'GraphSAGE'
    epochs: int = 100            # è®­ç»ƒè½®æ•°
    learning_rate: float = 0.001 # å­¦ä¹ ç‡
```

#### ä¸»è¦æ–¹æ³•

**build_stock_correlation_graph(returns_df: pd.DataFrame, method: str = "correlation") -> nx.Graph**
- æ„å»ºè‚¡ç¥¨å…³è”å›¾
- æ”¯æŒç›¸å…³æ€§ã€åç›¸å…³ã€äº’ä¿¡æ¯ä¸‰ç§æ–¹æ³•

**extract_graph_embeddings(node_features: pd.DataFrame) -> GraphEmbedding**
- æå–å›¾åµŒå…¥
- è¿”å›èŠ‚ç‚¹åµŒå…¥å’Œå›¾çº§åˆ«ç‰¹å¾

**detect_community_structures(method: str = "louvain") -> Dict[str, int]**
- æ£€æµ‹ç¤¾åŒºç»“æ„
- æ”¯æŒå¤šç§ç¤¾åŒºå‘ç°ç®—æ³•

**calculate_centrality_measures() -> pd.DataFrame**
- è®¡ç®—è¯¦ç»†çš„ä¸­å¿ƒæ€§åº¦é‡

**propagate_graph_signals(initial_signals: pd.Series) -> pd.Series**
- å›¾ä¿¡å·ä¼ æ’­
- å¯ç”¨äºä¿¡æ¯æ‰©æ•£åˆ†æ

#### ä½¿ç”¨ç¤ºä¾‹
```
from module_02_feature_engineering.graph_features.graph_embeddings import GraphConfig

# é…ç½®å›¾åµŒå…¥
config = GraphConfig(
    embedding_dim=32,
    gnn_type="GAT",
    epochs=50
)

extractor = GraphEmbeddingExtractor(config)

# æ„å»ºå›¾
graph = extractor.build_stock_correlation_graph(
    returns_df, 
    method="correlation"
)

# æå–åµŒå…¥
node_features = returns_df.describe().T
embedding_result = extractor.extract_graph_embeddings(node_features)

# ç¤¾åŒºå‘ç°
communities = extractor.detect_community_structures("louvain")
```

### FeatureDatabaseManager

ä¸“ç”¨çš„ç‰¹å¾æ•°æ®åº“ç®¡ç†å™¨ï¼Œè´Ÿè´£æ‰€æœ‰ç‰¹å¾æ•°æ®çš„æŒä¹…åŒ–å­˜å‚¨ã€‚

#### æ„é€ å‡½æ•°
```
FeatureDatabaseManager(db_path: str = "data/module02_features.db")
```

#### ä¸»è¦æ–¹æ³•

**æŠ€æœ¯æŒ‡æ ‡ç›¸å…³**
- `save_technical_indicators(symbol: str, indicators_df: pd.DataFrame) -> bool`
- `get_technical_indicators(symbol: str, start_date: str = None, end_date: str = None) -> pd.DataFrame`

**å› å­æ•°æ®ç›¸å…³**
- `save_factor_data(factor_id: str, symbol: str, factor_values: pd.Series, factor_type: str = "custom") -> bool`
- `get_factor_data(factor_id: str, symbol: str = None, start_date: str = None, end_date: str = None) -> pd.DataFrame`

**ç¥ç»å› å­ç›¸å…³**
- `save_neural_factor(factor_result: DiscoveredFactor) -> bool`
- `get_neural_factors(factor_id: str = None) -> List[Dict[str, Any]]`

**å›¾ç‰¹å¾ç›¸å…³**
- `save_graph_features(symbol: str, date: str, features: Dict[str, Any]) -> bool`
- `get_graph_features(symbol: str, start_date: str = None, end_date: str = None) -> pd.DataFrame`

**æ—¶é—´åºåˆ—ç‰¹å¾ç›¸å…³**
- `save_time_series_features(symbol: str, features_dict: Dict[str, Any]) -> bool`
- `get_time_series_features(symbol: str, start_date: str = None, end_date: str = None) -> pd.DataFrame`

**å›¾åµŒå…¥ç›¸å…³**
- `save_graph_embeddings(symbol: str, embeddings: np.ndarray, graph_config: Dict[str, Any] = None) -> bool`
- `get_graph_embeddings(symbol: str) -> Optional[np.ndarray]`

**æ•°æ®åº“ç®¡ç†**
- `get_database_stats() -> Dict[str, Any]`
- `cleanup_old_data(days_to_keep: int = 365) -> bool`

#### ä½¿ç”¨ç¤ºä¾‹
```
# è·å–ç‰¹å¾æ•°æ®åº“ç®¡ç†å™¨
from module_02_feature_engineering import get_feature_database_manager

feature_db = get_feature_database_manager()

# ä¿å­˜æŠ€æœ¯æŒ‡æ ‡
feature_db.save_technical_indicators("000001", indicators_df)

# æŸ¥è¯¢æŠ€æœ¯æŒ‡æ ‡
indicators = feature_db.get_technical_indicators("000001", "2024-01-01", "2024-12-01")

# æ•°æ®åº“ç»Ÿè®¡
stats = feature_db.get_database_stats()
print(f"æ•°æ®åº“å¤§å°: {stats['database_size_mb']:.2f} MB")
```

### FeatureCacheManager

ç‰¹å¾ç¼“å­˜ç®¡ç†å™¨ï¼Œæä¾›å†…å­˜çº§åˆ«çš„å¿«é€Ÿæ•°æ®è®¿é—®ã€‚

#### æ„é€ å‡½æ•°
```
FeatureCacheManager(max_size: int = 1000, ttl: int = 3600)
```

#### ä¸»è¦æ–¹æ³•

**set(data_type: str, symbol: str, data: Any, **kwargs) -> None**
- è®¾ç½®ç¼“å­˜æ•°æ®

**get(data_type: str, symbol: str, **kwargs) -> Optional[Any]**
- è·å–ç¼“å­˜æ•°æ®

**clear() -> None**
- æ¸…ç©ºæ‰€æœ‰ç¼“å­˜

**get_stats() -> Dict[str, Any]**
- è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯

#### ä½¿ç”¨ç¤ºä¾‹
```
cache = FeatureCacheManager(max_size=500, ttl=1800)

# ç¼“å­˜æŠ€æœ¯æŒ‡æ ‡
cache.set("technical_indicators", "000001", indicators_df)

# è·å–ç¼“å­˜
cached_indicators = cache.get("technical_indicators", "000001")
```

## ä¾¿æ·å‡½æ•°

### calculate_technical_indicators
å¿«é€Ÿè®¡ç®—æŠ€æœ¯æŒ‡æ ‡çš„ä¾¿æ·å‡½æ•°
```
from module_02_feature_engineering import calculate_technical_indicators

indicators = calculate_technical_indicators(ohlcv_data)
```

### discover_factors
å¿«é€Ÿå› å­å‘ç°çš„ä¾¿æ·å‡½æ•°
```
from module_02_feature_engineering import discover_factors

factors = discover_factors(features_df, returns_series)
```

### extract_graph_features
å¿«é€Ÿå›¾ç‰¹å¾æå–çš„ä¾¿æ·å‡½æ•°
```
from module_02_feature_engineering import extract_graph_features

graph_embedding = extract_graph_features(returns_df, node_features_df)
```

## æ•°æ®æµç¨‹å’Œé›†æˆ

### ä¸ Module 01 çš„é›†æˆ

Module 02 ä¸ Module 01 (æ•°æ®ç®¡é“) ç´§å¯†é›†æˆï¼Œå®ç°æ— ç¼çš„æ•°æ®æµè½¬ï¼š

```
# å…¸å‹çš„æ•°æ®å¤„ç†æµç¨‹
from module_01_data_pipeline import AkshareDataCollector, get_database_manager
from module_02_feature_engineering import *

# 1. ä» Module 01 è·å–åŸå§‹æ•°æ®
collector = AkshareDataCollector()
stock_data = collector.fetch_stock_history("000001", "20230101", "20241201")

# 2. Module 02 è¿›è¡Œç‰¹å¾å·¥ç¨‹
tech_indicators = calculate_technical_indicators(stock_data)

# 3. ä¿å­˜åˆ° Module 02 ä¸“ç”¨æ•°æ®åº“
feature_db = get_feature_database_manager()
feature_db.save_technical_indicators("000001", tech_indicators)

# 4. ä¸ºåç»­æ¨¡å—æä¾›ç‰¹å¾æ•°æ®
processed_features = feature_db.get_technical_indicators("000001")
```

### æ•°æ®åº“æ¶æ„

Module 02 ä½¿ç”¨ç‹¬ç«‹çš„ SQLite æ•°æ®åº“ (`data/module02_features.db`)ï¼ŒåŒ…å«ä»¥ä¸‹è¡¨ç»“æ„ï¼š

- **technical_indicators**: æŠ€æœ¯æŒ‡æ ‡æ•°æ®
- **factor_data**: å› å­æ•°æ®
- **neural_factors**: ç¥ç»å› å­ä¿¡æ¯
- **graph_features**: å›¾ç‰¹å¾æ•°æ®
- **time_series_features**: æ—¶é—´åºåˆ—ç‰¹å¾
- **graph_embeddings**: å›¾åµŒå…¥å‘é‡

### æ€§èƒ½ä¼˜åŒ–

- **å†…å­˜ç¼“å­˜**: ä½¿ç”¨ LRU ç¼“å­˜ç­–ç•¥ï¼Œå‡å°‘æ•°æ®åº“è®¿é—®
- **æ‰¹é‡å¤„ç†**: æ”¯æŒæ‰¹é‡ç‰¹å¾è®¡ç®—å’Œå­˜å‚¨
- **ç´¢å¼•ä¼˜åŒ–**: é’ˆå¯¹å¸¸ç”¨æŸ¥è¯¢æ¨¡å¼ä¼˜åŒ–æ•°æ®åº“ç´¢å¼•
- **å¼‚æ­¥å¤„ç†**: æ”¯æŒå¼‚æ­¥ç‰¹å¾è®¡ç®—ï¼ˆç¥ç»ç½‘ç»œéƒ¨åˆ†ï¼‰

## æµ‹è¯•å’Œç¤ºä¾‹

### è¿è¡Œå®Œæ•´æµ‹è¯•
```
cd /Users/victor/Desktop/25fininnov/FinLoom-server
python tests/module02_feature_engineering_test.py
```

è¯¥æµ‹è¯•åŒ…å«ï¼š
- æŠ€æœ¯æŒ‡æ ‡è®¡ç®—æµ‹è¯•
- å› å­åˆ†ææµ‹è¯•
- ç¥ç»å› å­å‘ç°æµ‹è¯•
- æ—¶é—´åºåˆ—ç‰¹å¾æå–æµ‹è¯•
- å›¾ç‰¹å¾åˆ†ææµ‹è¯•
- æ•°æ®åº“å­˜å‚¨å’ŒæŸ¥è¯¢æµ‹è¯•
- æ€§èƒ½åŸºå‡†æµ‹è¯•

### ç¤ºä¾‹æ•°æ®æº

æ‰€æœ‰ç¤ºä¾‹éƒ½ä½¿ç”¨æ¥è‡ª Module 01 çš„çœŸå®ä¸­å›½Aè‚¡æ•°æ®ï¼š
- **è‚¡ç¥¨ä»£ç **: 000001 (å¹³å®‰é“¶è¡Œ)ã€600036 (æ‹›å•†é“¶è¡Œ)ã€000858 (äº”ç²®æ¶²)
- **æ•°æ®èŒƒå›´**: 2023-01-01 åˆ° 2024-12-01
- **æ•°æ®ç±»å‹**: æ—¥é¢‘OHLCVæ•°æ®ã€åŸºæœ¬é¢æ•°æ®ã€å®è§‚æ•°æ®

## é«˜çº§åŠŸèƒ½

### å¤šå› å­æ¨¡å‹æ„å»º

```
# æ„å»ºå¤šå› å­æ¨¡å‹
from module_02_feature_engineering import FactorAnalyzer, NeuralFactorDiscovery

analyzer = FactorAnalyzer()
discoverer = NeuralFactorDiscovery(config)

# ä¼ ç»Ÿå› å­
traditional_factors = {
    'momentum': tech_indicators['rsi'],
    'value': pe_ratio_series,
    'quality': roe_series
}

# ç¥ç»å› å­
neural_factors = discoverer.discover_neural_factors(features, returns)

# ç»„åˆåˆ†æ
for name, factor in traditional_factors.items():
    result = analyzer.analyze_factor(factor, returns)
    print(f"{name}å› å­ IC: {result.ic:.4f}")
```

### å›¾ç½‘ç»œåˆ†æ

```
# è‚¡ç¥¨å…³è”ç½‘ç»œåˆ†æ
from module_02_feature_engineering import GraphEmbeddingExtractor

extractor = GraphEmbeddingExtractor()
graph = extractor.build_stock_correlation_graph(returns_matrix)

# ç¤¾åŒºå‘ç°
communities = extractor.detect_community_structures()

# ä¸­å¿ƒæ€§åˆ†æ
centrality_df = extractor.calculate_centrality_measures()

# ä¿¡å·ä¼ æ’­
initial_signals = pd.Series([1.0, 0.0, 0.0], index=stock_symbols)
propagated = extractor.propagate_graph_signals(initial_signals)
```

### å› å­æœ‰æ•ˆæ€§åˆ†æ

```
# å¤šå‘¨æœŸå› å­æœ‰æ•ˆæ€§åˆ†æ
effectiveness = discoverer.evaluate_factor_effectiveness(
    factor_values=factor_series,
    forward_returns=returns_series,
    periods=[1, 5, 10, 20]  # 1æ—¥ã€5æ—¥ã€10æ—¥ã€20æ—¥æŒæœ‰æœŸ
)

print("å› å­æœ‰æ•ˆæ€§åˆ†æ:")
for period in [1, 5, 10, 20]:
    ic = effectiveness.get(f'ic_{period}d', 0)
    ir = effectiveness.get(f'ir_{period}d', 0)
    print(f"{period}æ—¥æŒæœ‰æœŸ - IC: {ic:.4f}, IR: {ir:.4f}")
```

## é”™è¯¯å¤„ç†å’Œæ—¥å¿—

æ¨¡å—ä½¿ç”¨ç»Ÿä¸€çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—ç³»ç»Ÿï¼š

```
from common.exceptions import DataError, ModelError
from common.logging_system import setup_logger

# é”™è¯¯å¤„ç†ç¤ºä¾‹
try:
    factors = discover_factors(features, returns)
except DataError as e:
    logger.error(f"æ•°æ®é”™è¯¯: {e}")
except ModelError as e:
    logger.error(f"æ¨¡å‹é”™è¯¯: {e}")
```

### è°ƒè¯•æ¨¡å¼

```
import logging
logging.basicConfig(level=logging.DEBUG)

# å¯ç”¨è¯¦ç»†æ—¥å¿—
calculator = TechnicalIndicators()
indicators = calculator.calculate_all_indicators(stock_data)
```

## é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡
- `MODULE02_DB_PATH`: ç‰¹å¾æ•°æ®åº“è·¯å¾„
- `MODULE02_CACHE_SIZE`: ç¼“å­˜å¤§å°é™åˆ¶
- `MODULE02_LOG_LEVEL`: æ—¥å¿—çº§åˆ«

### ç‰¹å¾æ•°æ®åº“é…ç½®
é»˜è®¤ä½¿ç”¨ SQLite æ•°æ®åº“ï¼Œæ–‡ä»¶ä½äº `data/module02_features.db`ã€‚å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è‡ªå®šä¹‰ï¼š

```
feature_db = get_feature_database_manager("custom/path/features.db")
```

### ç¥ç»ç½‘ç»œé…ç½®

ç¥ç»å› å­å‘ç°å’Œå›¾åµŒå…¥åŠŸèƒ½éœ€è¦ PyTorch ç¯å¢ƒï¼š

```bash
# å®‰è£… PyTorch (CPUç‰ˆæœ¬)
pip install torch torchvision torchaudio

# å›¾ç¥ç»ç½‘ç»œåŠŸèƒ½éœ€è¦é¢å¤–ä¾èµ–
pip install torch-geometric
pip install networkx
```

## æ¥å£ç±»å‹è¯´æ˜

### ç¼–ç¨‹æ¥å£ (Programmatic API)
Module 02 æä¾›**ç¼–ç¨‹æ¥å£**ï¼Œå³ Python å‡½æ•°å’Œç±»çš„ç›´æ¥è°ƒç”¨æ¥å£ï¼š
- ç‰¹å¾è®¡ç®—ç±»ï¼š`TechnicalIndicators`ã€`TimeSeriesFeatures`
- å› å­åˆ†æç±»ï¼š`FactorAnalyzer`ã€`NeuralFactorDiscovery`
- å›¾åˆ†æç±»ï¼š`GraphAnalyzer`ã€`GraphEmbeddingExtractor`
- å­˜å‚¨ç®¡ç†ç±»ï¼š`FeatureDatabaseManager`ã€`FeatureCacheManager`
- ä¾¿æ·å‡½æ•°ï¼š`calculate_technical_indicators`ã€`discover_factors`ã€`extract_graph_features`

### æ•°æ®æœåŠ¡æ¥å£
Module 02 **ä¸æä¾›** REST API æ¥å£ã€‚å®ƒä½œä¸ºç‰¹å¾å·¥ç¨‹åŸºç¡€è®¾æ–½ï¼Œä¸ºå…¶ä»–æ¨¡å—æä¾›æ•°æ®å’Œè®¡ç®—æœåŠ¡ï¼š
- **Module 03 (AIæ¨¡å‹)**: æä¾›è®­ç»ƒç‰¹å¾å’Œé¢„æµ‹ç‰¹å¾
- **Module 04 (å¸‚åœºåˆ†æ)**: æä¾›å®æ—¶ç‰¹å¾è®¡ç®—æœåŠ¡
- **Module 05 (é£é™©ç®¡ç†)**: æä¾›é£é™©å› å­å’Œç‰¹å¾æ•°æ®
- **Module 09 (å›æµ‹)**: æä¾›å†å²ç‰¹å¾æ•°æ®ç”¨äºå›æµ‹åˆ†æ

### æ¨¡å—é—´æ•°æ®æµ

```
graph TB
    M01[Module 01<br/>Data Pipeline] --> M02[Module 02<br/>Feature Engineering]
    M02 --> M03[Module 03<br/>AI Models]
    M02 --> M04[Module 04<br/>Market Analysis]
    M02 --> M05[Module 05<br/>Risk Management]
    M02 --> M09[Module 09<br/>Backtesting]
    M02 --> M11[Module 11<br/>Visualization]
```

## æ€§èƒ½åŸºå‡†

### è®¡ç®—æ€§èƒ½

| æ“ä½œ | æ•°æ®è§„æ¨¡ | å¤„ç†æ—¶é—´ | å†…å­˜ä½¿ç”¨ |
|------|----------|----------|----------|
| æŠ€æœ¯æŒ‡æ ‡è®¡ç®— | 1000æ¡è®°å½• | ~50ms | ~10MB |
| å› å­åˆ†æ | 10ä¸ªå› å­Ã—1000æ¡ | ~200ms | ~20MB |
| ç¥ç»å› å­å‘ç° | 100ä¸ªç‰¹å¾Ã—1000æ¡ | ~30s | ~500MB |
| å›¾ç‰¹å¾æå– | 50åªè‚¡ç¥¨Ã—250å¤© | ~2s | ~100MB |
| å›¾åµŒå…¥è®­ç»ƒ | 50åªè‚¡ç¥¨Ã—250å¤© | ~60s | ~800MB |

### å­˜å‚¨æ€§èƒ½

| æ“ä½œ | æ•°æ®è§„æ¨¡ | å­˜å‚¨æ—¶é—´ | å­˜å‚¨ç©ºé—´ |
|------|----------|----------|----------|
| æŠ€æœ¯æŒ‡æ ‡å­˜å‚¨ | 1åªè‚¡ç¥¨Ã—250å¤©Ã—20æŒ‡æ ‡ | ~100ms | ~50KB |
| å› å­æ•°æ®å­˜å‚¨ | 10ä¸ªå› å­Ã—1000æ¡ | ~50ms | ~30KB |
| ç¥ç»å› å­å­˜å‚¨ | 1ä¸ªå› å­æ¨¡å‹ | ~10ms | ~1MB |
| å›¾åµŒå…¥å­˜å‚¨ | 50åªè‚¡ç¥¨Ã—64ç»´ | ~20ms | ~100KB |

### æ‰©å±•æ€§

- **è‚¡ç¥¨æ•°é‡**: æ”¯æŒ1000+åªè‚¡ç¥¨çš„å¹¶è¡Œå¤„ç†
- **å†å²æ•°æ®**: æ”¯æŒ10å¹´+çš„å†å²æ•°æ®ç‰¹å¾è®¡ç®—
- **ç‰¹å¾ç»´åº¦**: æ”¯æŒ1000+ç»´ç‰¹å¾çš„ç¥ç»ç½‘ç»œè®­ç»ƒ
- **å¹¶å‘è®¿é—®**: æ”¯æŒå¤šè¿›ç¨‹å¹¶å‘ç‰¹å¾è®¡ç®—å’Œå­˜å‚¨

## æ€»ç»“

Module 02 ç‰¹å¾å·¥ç¨‹æ¨¡å—å·²ç»å®Œå…¨å®ç°äº†ç°ä»£é‡åŒ–æŠ•èµ„ç³»ç»Ÿæ‰€éœ€çš„ç‰¹å¾å·¥ç¨‹åŠŸèƒ½ï¼š

### åŠŸèƒ½å®Œæ•´æ€§ âœ…
- âœ“ ç»å…¸æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ï¼ˆç§»åŠ¨å¹³å‡ã€RSIã€MACDç­‰ï¼‰
- âœ“ æ™ºèƒ½å› å­å‘ç°ï¼ˆä¼ ç»Ÿåˆ†æ+ç¥ç»ç½‘ç»œï¼‰
- âœ“ æ—¶é—´åºåˆ—ç‰¹å¾å·¥ç¨‹ï¼ˆåŠ¨é‡ã€æ³¢åŠ¨ç‡ã€è¶‹åŠ¿ï¼‰
- âœ“ å›¾ç½‘ç»œç‰¹å¾åˆ†æï¼ˆå…³è”åˆ†æ+å›¾åµŒå…¥ï¼‰
- âœ“ å®Œå–„çš„æ•°æ®å­˜å‚¨ç®¡ç†ï¼ˆä¸“ç”¨æ•°æ®åº“+å†…å­˜ç¼“å­˜ï¼‰

### æŠ€æœ¯å…ˆè¿›æ€§ âœ…
- âœ“ **æ·±åº¦å­¦ä¹ **: ç¥ç»å› å­å‘ç°ã€å›¾ç¥ç»ç½‘ç»œåµŒå…¥
- âœ“ **å›¾ç½‘ç»œåˆ†æ**: è‚¡ç¥¨å…³è”ç½‘ç»œã€ç¤¾åŒºå‘ç°ã€ä¿¡å·ä¼ æ’­
- âœ“ **å¤šå°ºåº¦åˆ†æ**: æ”¯æŒå¤šæ—¶é—´çª—å£ã€å¤šå‘¨æœŸç‰¹å¾æå–
- âœ“ **è‡ªåŠ¨åŒ–ç¨‹åº¦**: ä¸€é”®è®¡ç®—æ‰€æœ‰ç‰¹å¾ï¼Œæ™ºèƒ½å› å­å‘ç°

### å·¥ç¨‹è´¨é‡ âœ…
- âœ“ **æ•°æ®é›†æˆ**: ä¸Module01æ— ç¼é›†æˆï¼ŒçœŸå®æ•°æ®å¤„ç†
- âœ“ **æ€§èƒ½ä¼˜åŒ–**: å†…å­˜ç¼“å­˜ã€æ‰¹é‡å¤„ç†ã€å¼‚æ­¥è®¡ç®—
- âœ“ **é”™è¯¯å¤„ç†**: å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œæ—¥å¿—ç³»ç»Ÿ
- âœ“ **å¯æ‰©å±•æ€§**: æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ·»åŠ æ–°ç‰¹å¾ç±»å‹

### å®ç”¨æ€§ âœ…
- âœ“ **å³æ’å³ç”¨**: ä¸°å¯Œçš„ä¾¿æ·å‡½æ•°ï¼Œç®€åŒ–APIè°ƒç”¨
- âœ“ **æ•°æ®æŒä¹…åŒ–**: ä¸“ç”¨æ•°æ®åº“ï¼Œæ”¯æŒå†å²æ•°æ®æŸ¥è¯¢
- âœ“ **æ€§èƒ½ç›‘æ§**: è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯å’Œæ€§èƒ½åŸºå‡†
- âœ“ **æ–‡æ¡£å®Œæ•´**: å…¨é¢çš„APIæ–‡æ¡£å’Œä½¿ç”¨ç¤ºä¾‹

**ç»“è®º**: Module 02 å·²ç»å®Œå…¨èƒ½å¤Ÿæ»¡è¶³é‡åŒ–äº¤æ˜“ç³»ç»Ÿä¸­ç‰¹å¾å·¥ç¨‹çš„æ‰€æœ‰éœ€æ±‚ï¼Œæä¾›äº†ä»åŸºç¡€æŠ€æœ¯æŒ‡æ ‡åˆ°é«˜çº§ç¥ç»ç½‘ç»œç‰¹å¾çš„å®Œæ•´è§£å†³æ–¹æ¡ˆã€‚å®ƒé€šè¿‡ç¼–ç¨‹æ¥å£ä¸ºå…¶ä»–æ¨¡å—æä¾›æœåŠ¡ï¼Œå½¢æˆäº†å®Œæ•´çš„ç‰¹å¾å·¥ç¨‹åŸºç¡€è®¾æ–½ã€‚

## å®Œæ•´çš„å·¥ä½œæµç¨‹ç¤ºä¾‹

ä»¥ä¸‹æ˜¯ä¸€ä¸ªå®Œæ•´çš„çœŸå®ä¸–ç•Œä½¿ç”¨æ¡ˆä¾‹ï¼Œä»æ•°æ®è·å–åˆ°ç‰¹å¾å·¥ç¨‹å†åˆ°ç»“æœå­˜å‚¨ï¼š

```
#!/usr/bin/env python3
"""
å®Œæ•´çš„Module 02ç‰¹å¾å·¥ç¨‹å®æˆ˜æ¼”ç¤º
æ¼”ç¤ºå¦‚ä½•ä»çœŸå®æ•°æ®åˆ°å®Œæ•´çš„ç‰¹å¾å·¥ç¨‹ç®¡é“
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# å¯¼å…¥æ¨¡å—
from module_01_data_pipeline import AkshareDataCollector, get_database_manager
from module_02_feature_engineering import (
    TechnicalIndicators, calculate_technical_indicators,
    FactorAnalyzer, TimeSeriesFeatures, GraphAnalyzer,
    get_feature_database_manager, FeatureCacheManager
)

def complete_feature_engineering_pipeline():
    """å®Œæ•´çš„ç‰¹å¾å·¥ç¨‹ç®¡é“æ¼”ç¤º"""
    
    # ==== ç¬¬1æ­¥: æ•°æ®è·å– (æ¥è‡ª Module 01) ====
    print("ğŸš€ æ­¥éª¤1: ä» Module 01 è·å–çœŸå®æ•°æ®")
    
    collector = AkshareDataCollector(rate_limit=0.5)
    # é€‰å–ä¸åŒè¡Œä¸šçš„ä»£è¡¨æ€§è‚¡ç¥¨
    symbols = {
        "000001": "å¹³å®‰é“¶è¡Œ",  # é‡‘èä¸š
        "600036": "æ‹›å•†é“¶è¡Œ",  # é‡‘èä¸š
        "000858": "äº”ç²®æ¶²",    # æ¶ˆè´¹å“
        "000002": "ä¸‡ ç§‘A",    # æˆ¿åœ°äº§
        "600519": "è´µå·èŒ…å°"   # æ¶ˆè´¹å“
    }
    
    end_date = datetime.now().strftime("%Y%m%d")
    start_date = (datetime.now() - timedelta(days=90)).strftime("%Y%m%d")  # 3ä¸ªæœˆæ•°æ®
    
    # æ”¶é›†æ‰€æœ‰è‚¡ç¥¨çš„å†å²æ•°æ®
    stock_data = {}
    for symbol, name in symbols.items():
        try:
            data = collector.fetch_stock_history(symbol, start_date, end_date)
            if not data.empty and len(data) > 20:  # è‡³å°‘20å¤©æ•°æ®
                stock_data[symbol] = data
                print(f"âœ“ {symbol} ({name}): {len(data)} æ¡è®°å½•")
            else:
                print(f"âš ï¸ {symbol} ({name}): æ•°æ®ä¸è¶³")
        except Exception as e:
            print(f"âŒ {symbol} ({name}): è·å–å¤±è´¥ - {e}")
    
    if len(stock_data) < 3:
        print("âŒ å¯ç”¨æ•°æ®ä¸è¶³ï¼Œéœ€è¦è‡³å°‘3åªè‚¡ç¥¨")
        return False
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(stock_data)} åªè‚¡ç¥¨æ•°æ®")
    
    # ==== ç¬¬2æ­¥: æŠ€æœ¯æŒ‡æ ‡è®¡ç®— ====
    print(f"\nğŸ“Š æ­¥éª¤2: æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å’Œå­˜å‚¨")
    
    tech_calculator = TechnicalIndicators()
    feature_db = get_feature_database_manager()
    
    technical_results = {}
    for symbol, data in stock_data.items():
        # è®¡ç®—æ‰€æœ‰æŠ€æœ¯æŒ‡æ ‡
        indicators = tech_calculator.calculate_all_indicators(data)
        technical_results[symbol] = indicators
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        if feature_db.save_technical_indicators(symbol, indicators):
            print(f"âœ“ {symbol}: æŠ€æœ¯æŒ‡æ ‡å·²ä¿å­˜ ({indicators.shape[1]} ä¸ªæŒ‡æ ‡)")
        else:
            print(f"âš ï¸ {symbol}: æŠ€æœ¯æŒ‡æ ‡ä¿å­˜å¤±è´¥")
    
    # ==== ç¬¬3æ­¥: å› å­åˆ†æ ====
    print(f"\nğŸ” æ­¥éª¤3: å› å­åˆ†æå’Œæœ‰æ•ˆæ€§è¯„ä¼°")
    
    factor_analyzer = FactorAnalyzer()
    factor_results = {}
    
    for symbol, indicators in technical_results.items():
        try:
            # è®¡ç®—æ”¶ç›Šç‡
            returns = indicators['close'].pct_change().dropna()
            
            # é€‰å–å‡ ä¸ªæŠ€æœ¯æŒ‡æ ‡ä½œä¸ºå› å­
            factors_to_test = {
                'rsi': indicators['rsi'],
                'macd': indicators['macd'],
                'bb_position': (indicators['close'] - indicators['bb_lower']) / 
                              (indicators['bb_upper'] - indicators['bb_lower'])  # å¸ƒæ—å¸¦ä½ç½®
            }
            
            symbol_factors = {}
            for factor_name, factor_values in factors_to_test.items():
                # å¯¹é½æ•°æ®
                common_index = factor_values.index.intersection(returns.index)
                if len(common_index) > 10:
                    factor_aligned = factor_values.loc[common_index]
                    returns_aligned = returns.loc[common_index]
                    
                    # å› å­åˆ†æ
                    result = factor_analyzer.analyze_factor(factor_aligned, returns_aligned)
                    symbol_factors[factor_name] = result
                    
                    # ä¿å­˜å› å­æ•°æ®
                    factor_id = f"{factor_name}_factor_{symbol}"
                    feature_db.save_factor_data(factor_id, symbol, factor_aligned, "technical")
                    
                    print(f"âœ“ {symbol} {factor_name}: IC={result.ic:.4f}, Rank IC={result.rank_ic:.4f}")
            
            factor_results[symbol] = symbol_factors
            
        except Exception as e:
            print(f"âš ï¸ {symbol}: å› å­åˆ†æå¤±è´¥ - {e}")
    
    # ==== ç¬¬4æ­¥: æ—¶é—´åºåˆ—ç‰¹å¾ ====
    print(f"\nâ° æ­¥éª¤4: æ—¶é—´åºåˆ—ç‰¹å¾æå–")
    
    ts_extractor = TimeSeriesFeatures()
    
    for symbol, data in stock_data.items():
        try:
            # æå–æ—¶é—´åºåˆ—ç‰¹å¾
            ts_features = ts_extractor.extract_all_features(data['close'])
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            if feature_db.save_time_series_features(symbol, ts_features):
                print(f"âœ“ {symbol}: æ—¶é—´åºåˆ—ç‰¹å¾å·²ä¿å­˜ ({len(ts_features)} ä¸ªç‰¹å¾)")
            
        except Exception as e:
            print(f"âš ï¸ {symbol}: æ—¶é—´åºåˆ—ç‰¹å¾æå–å¤±è´¥ - {e}")
    
    # ==== ç¬¬5æ­¥: å›¾ç‰¹å¾åˆ†æ ====
    print(f"\nğŸ”— æ­¥éª¤5: è‚¡ç¥¨å…³è”ç½‘ç»œåˆ†æ")
    
    # æ„å»ºæ”¶ç›Šç‡çŸ©é˜µ
    returns_matrix = pd.DataFrame()
    for symbol, data in stock_data.items():
        returns_matrix[symbol] = data['close'].pct_change()
    
    returns_matrix = returns_matrix.dropna()
    
    if not returns_matrix.empty and len(returns_matrix.columns) >= 2:
        try:
            graph_analyzer = GraphAnalyzer()
            graph_features = graph_analyzer.extract_graph_features(returns_matrix)
            
            print(f"âœ“ å›¾ç‰¹å¾æå–æˆåŠŸ: {len(graph_features)} ä¸ªç‰¹å¾")
            
            # ä¿å­˜å›¾ç‰¹å¾
            test_date = returns_matrix.index[-1].strftime('%Y-%m-%d')  # ä½¿ç”¨æœ€æ–°æ—¥æœŸ
            
            for symbol in returns_matrix.columns:
                symbol_features = {}
                for feature_name, feature_obj in graph_features.items():
                    if hasattr(feature_obj, 'values') and symbol in feature_obj.values:
                        values = feature_obj.values[symbol]
                        if isinstance(values, dict):
                            symbol_features.update(values)
                        else:
                            symbol_features[feature_name] = values
                
                if symbol_features:
                    feature_db.save_graph_features(symbol, test_date, symbol_features)
                    print(f"âœ“ {symbol}: å›¾ç‰¹å¾å·²ä¿å­˜ ({len(symbol_features)} ä¸ªç‰¹å¾)")
            
        except Exception as e:
            print(f"âš ï¸ å›¾ç‰¹å¾åˆ†æå¤±è´¥: {e}")
    
    # ==== ç¬¬6æ­¥: ç»Ÿè®¡æ€»ç»“ ====
    print(f"\nğŸ“Š æ­¥éª¤6: ç‰¹å¾å·¥ç¨‹ç»Ÿè®¡æ€»ç»“")
    
    # è·å–æ•°æ®åº“ç»Ÿè®¡
    stats = feature_db.get_database_stats()
    
    print(f"ğŸ“ æ•°æ®åº“ç»Ÿè®¡:")
    print(f"  â€¢ æ•°æ®åº“å¤§å°: {stats.get('database_size_mb', 0):.2f} MB")
    print(f"  â€¢ æŠ€æœ¯æŒ‡æ ‡è®°å½•: {stats.get('technical_indicators_count', 0):,}")
    print(f"  â€¢ å› å­æ•°æ®è®°å½•: {stats.get('factor_data_count', 0):,}")
    print(f"  â€¢ æ—¶é—´åºåˆ—ç‰¹å¾: {stats.get('time_series_features_count', 0):,}")
    print(f"  â€¢ å›¾ç‰¹å¾è®°å½•: {stats.get('graph_features_count', 0):,}")
    print(f"  â€¢ æ¶‰åŠè‚¡ç¥¨æ•°é‡: {stats.get('unique_symbols', 0)} åª")
    
    # å±•ç¤ºæœ€ä½³å› å­
    print(f"\nğŸ† æœ€ä½³å› å­æ’è¡Œ:")
    all_factors = []
    for symbol, factors in factor_results.items():
        for factor_name, result in factors.items():
            all_factors.append({
                'symbol': symbol,
                'factor': factor_name,
                'ic': result.ic,
                'rank_ic': result.rank_ic,
                'ir': result.ir
            })
    
    if all_factors:
        factor_df = pd.DataFrame(all_factors)
        top_factors = factor_df.nlargest(5, 'ic')
        
        for idx, row in top_factors.iterrows():
            print(f"  {idx+1}. {row['symbol']} - {row['factor']}: IC={row['ic']:.4f}")
    
    print(f"\nâœ… ç‰¹å¾å·¥ç¨‹ç®¡é“æ‰§è¡Œå®Œæˆ!")
    return True

if __name__ == "__main__":
    complete_feature_engineering_pipeline()
```

è¿™ä¸ªå®Œæ•´çš„å·¥ä½œæµç¨‹æ¼”ç¤ºäº†ï¼š

1. **çœŸå®æ•°æ®é›†æˆ**: ä» Module 01 è·å–çœŸå®çš„ä¸­å›½Aè‚¡æ•°æ®
2. **æŠ€æœ¯æŒ‡æ ‡è®¡ç®—**: è®¡ç®—å…¨å¥—æŠ€æœ¯æŒ‡æ ‡å¹¶å­˜å‚¨åˆ°æ•°æ®åº“
3. **å› å­åˆ†æ**: è¯„ä¼°å› å­æœ‰æ•ˆæ€§å¹¶ä¿å­˜å› å­æ•°æ®
4. **æ—¶é—´åºåˆ—ç‰¹å¾**: æå–åŠ¨é‡ã€æ³¢åŠ¨ç‡ã€è¶‹åŠ¿ç‰¹å¾
5. **å›¾ç½‘ç»œåˆ†æ**: åˆ†æè‚¡ç¥¨é—´å…³è”å…³ç³»
6. **æ•°æ®æŒä¹…åŒ–**: æ‰€æœ‰ç‰¹å¾æ•°æ®ä¿å­˜åˆ° Module 02 ä¸“ç”¨æ•°æ®åº“
7. **ç»“æœç»Ÿè®¡**: æä¾›è¯¦ç»†çš„å¤„ç†ç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### æ‰§è¡Œè¿™ä¸ªå®Œæ•´ç¤ºä¾‹

```
cd /Users/victor/Desktop/25fininnov/FinLoom-server
python -c "from module_02_feature_engineering.module02_README import complete_feature_engineering_pipeline; complete_feature_engineering_pipeline()"
```

æˆ–è€…å°†ä»£ç ä¿å­˜ä¸ºç‹¬ç«‹æ–‡ä»¶è¿è¡Œã€‚

### æ€§èƒ½åŸºå‡†å’Œé¢„æœŸç»“æœ

è¿è¡Œä¸Šè¿°å®Œæ•´æµç¨‹åï¼Œæ‚¨å¯ä»¥æœŸå¾…ï¼š

- **å¤„ç†é€Ÿåº¦**: 5åªè‚¡ç¥¨90å¤©æ•°æ®çº¦éœ€2-3åˆ†é’Ÿ
- **å­˜å‚¨ç©ºé—´**: çº¦0.5-1MBæ•°æ®åº“æ–‡ä»¶
- **ç‰¹å¾æ•°é‡**: æ¯åªè‚¡ç¥¨çº¦50+ä¸ªæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
- **å› å­è´¨é‡**: ICå€¼é€šå¸¸åœ¨-0.3åˆ°0.3ä¹‹é—´
- **æ•°æ®å®Œæ•´æ€§**: 99%+çš„æ•°æ®æˆåŠŸä¿å­˜

# Module 02 ç‰¹å¾å·¥ç¨‹æ¨¡å— API æ–‡æ¡£

## æ¨¡å—æ¦‚è§ˆ

Module 02 ç‰¹å¾å·¥ç¨‹æ¨¡å—æ˜¯ FinLoom é‡åŒ–äº¤æ˜“ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œæä¾›å…¨é¢çš„é‡‘èæ•°æ®ç‰¹å¾æå–ã€å› å­å‘ç°å’Œç‰¹å¾ç®¡ç†åŠŸèƒ½ã€‚æœ¬æ¨¡å—ä» Module 01 è·å–æ¸…æ´—åçš„æ•°æ®ï¼Œè¿›è¡Œæ·±åº¦ç‰¹å¾å·¥ç¨‹å¤„ç†ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°ä¸“ç”¨çš„ SQLite æ•°æ®åº“ä¸­ã€‚

### ä¸»è¦åŠŸèƒ½

- **æŠ€æœ¯æŒ‡æ ‡è®¡ç®—**ï¼šæ”¯æŒ 20+ ç§å¸¸ç”¨æŠ€æœ¯æŒ‡æ ‡
- **å› å­å‘ç°**ï¼šåŒ…æ‹¬ä¼ ç»Ÿå› å­åˆ†æã€ç¥ç»ç½‘ç»œå› å­å‘ç°å’Œé—ä¼ ç®—æ³•å› å­æœç´¢
- **æ—¶é—´åºåˆ—ç‰¹å¾**ï¼šæå–æ—¶åºæ¨¡å¼ã€å­£èŠ‚æ€§ç‰¹å¾å’Œåˆ¶åº¦è¯†åˆ«
- **å›¾ç‰¹å¾åˆ†æ**ï¼šåŸºäºè‚¡ç¥¨å…³ç³»ç½‘ç»œçš„å›¾ç‰¹å¾å’Œç¤¾åŒºæ£€æµ‹
- **ç‰¹å¾å­˜å‚¨ç®¡ç†**ï¼šä¸“ç”¨æ•°æ®åº“å’Œç¼“å­˜ç³»ç»Ÿ
- **æ·±åº¦å­¦ä¹ ç‰¹å¾**ï¼šè‡ªåŠ¨ç¼–ç å™¨å’ŒLSTMç‰¹å¾æå–

## æ ¸å¿ƒç±»å’Œæ¥å£

### 1. ç‰¹å¾å·¥ç¨‹ä¸»æµæ°´çº¿

```
from module_02_feature_engineering import FeatureEngineeringPipeline

# åˆå§‹åŒ–ä¸»æµæ°´çº¿
pipeline = FeatureEngineeringPipeline()

# å¤„ç†ç‰¹å¾
features = pipeline.process_features(data)
# è¿”å›: {
#   'technical': DataFrame,      # æŠ€æœ¯æŒ‡æ ‡
#   'time_series': DataFrame,    # æ—¶åºç‰¹å¾  
#   'graph': DataFrame          # å›¾ç‰¹å¾
# }
```

### 2. æŠ€æœ¯æŒ‡æ ‡è®¡ç®—

```
from module_02_feature_engineering import TechnicalIndicators, calculate_technical_indicators

# åˆå§‹åŒ–æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å™¨
calculator = TechnicalIndicators()

# å•ä¸ªæŒ‡æ ‡è®¡ç®—
sma_20 = calculator.calculate_sma(data['close'], window=20)
rsi = calculator.calculate_rsi(data['close'], window=14)
macd_data = calculator.calculate_macd(data['close'])

# æ‰¹é‡è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
all_indicators = calculator.calculate_all_indicators(data)

# ä¾¿æ·å‡½æ•°
indicators = calculate_technical_indicators(data)
```

#### æ”¯æŒçš„æŠ€æœ¯æŒ‡æ ‡

| æŒ‡æ ‡ç±»åˆ« | æŒ‡æ ‡åç§° | å‡½æ•°å | å‚æ•° |
|---------|---------|--------|------|
| è¶‹åŠ¿æŒ‡æ ‡ | ç®€å•ç§»åŠ¨å¹³å‡ | `calculate_sma(close, window)` | window=20 |
| | æŒ‡æ•°ç§»åŠ¨å¹³å‡ | `calculate_ema(close, span)` | span=12 |
| | å¸ƒæ—å¸¦ | `calculate_bollinger_bands(close, window, std)` | window=20, std=2 |
| åŠ¨é‡æŒ‡æ ‡ | RSI | `calculate_rsi(close, window)` | window=14 |
| | MACD | `calculate_macd(close, fast, slow, signal)` | fast=12, slow=26, signal=9 |
| | éšæœºæŒ‡æ ‡ | `calculate_stochastic(high, low, close, k, d)` | k=14, d=3 |
| æˆäº¤é‡æŒ‡æ ‡ | OBV | `calculate_obv(close, volume)` | - |
| | æˆäº¤é‡åŠ æƒå¹³å‡ä»· | `calculate_vwap(high, low, close, volume)` | - |
| æ³¢åŠ¨ç‡æŒ‡æ ‡ | ATR | `calculate_atr(high, low, close, window)` | window=14 |
| | å†å²æ³¢åŠ¨ç‡ | `calculate_historical_volatility(close, window)` | window=20 |

### 3. å› å­åˆ†æå’Œå‘ç°

#### 3.1 ä¼ ç»Ÿå› å­åˆ†æ

```
from module_02_feature_engineering.factor_discovery import FactorAnalyzer, FactorConfig

# é…ç½®å› å­åˆ†æå‚æ•°
config = FactorConfig(
    lookback_period=252,
    forward_period=5,
    min_periods=60,
    neutralize=False,
    standardize=True
)

# åˆå§‹åŒ–åˆ†æå™¨
analyzer = FactorAnalyzer()

# åˆ†æå› å­
factor_result = analyzer.analyze_factor(factor_values, returns)
# è¿”å›: FactorResult(
#   factor_name=str,
#   factor_values=Series,
#   ic=float,           # ä¿¡æ¯ç³»æ•°
#   ir=float,           # ä¿¡æ¯æ¯”ç‡
#   rank_ic=float,      # æ’åºä¿¡æ¯ç³»æ•°
#   turnover=float,     # æ¢æ‰‹ç‡
#   decay=float         # è¡°å‡ç‡
# )
```

#### 3.2 å› å­è¯„ä¼°

```
from module_02_feature_engineering.factor_discovery import FactorEvaluator, FactorEvaluationConfig

# é…ç½®è¯„ä¼°å‚æ•°
config = FactorEvaluationConfig(
    rolling_window=252,
    min_periods=60,
    quantiles=5,
    forward_periods=[1, 5, 10, 20]
)

# åˆå§‹åŒ–è¯„ä¼°å™¨
evaluator = FactorEvaluator(config)

# è¯„ä¼°å› å­
result = evaluator.evaluate_factor(factor_values, forward_returns, "factor_name")
# è¿”å›: FactorEvaluationResult åŒ…å«å®Œæ•´çš„è¯„ä¼°æŒ‡æ ‡
```

#### 3.3 ç¥ç»ç½‘ç»œå› å­å‘ç°

```
from module_02_feature_engineering.factor_discovery import NeuralFactorDiscovery, NeuralConfig

# é…ç½®ç¥ç»ç½‘ç»œå‚æ•°
config = NeuralConfig(
    input_dim=10,
    hidden_layers=[64, 32],
    output_dim=1,
    learning_rate=0.001,
    batch_size=32,
    max_epochs=100
)

# åˆå§‹åŒ–ç¥ç»å› å­å‘ç°
discoverer = NeuralFactorDiscovery(config)

# å‘ç°å› å­
factors = discoverer.discover_factors(features_data, target_returns)
```

#### 3.4 é—ä¼ ç®—æ³•å› å­æœç´¢

```
from module_02_feature_engineering.factor_discovery import GeneticFactorSearch, GeneticConfig

# é…ç½®é—ä¼ ç®—æ³•å‚æ•°
config = GeneticConfig(
    population_size=50,
    generations=100,
    crossover_rate=0.8,
    mutation_rate=0.1
)

# åˆå§‹åŒ–é—ä¼ æœç´¢
searcher = GeneticFactorSearch(config)

# æœç´¢æœ€ä¼˜å› å­
best_gene, fitness = searcher.search(data, target, fitness_func)
```

### 4. æ—¶é—´åºåˆ—ç‰¹å¾

#### 4.1 åŸºç¡€æ—¶åºç‰¹å¾

```
from module_02_feature_engineering.temporal_features import TimeSeriesFeatures, TimeSeriesConfig

# é…ç½®æ—¶åºç‰¹å¾å‚æ•°
config = TimeSeriesConfig(
    window_sizes=[5, 10, 20, 60],
    lag_periods=[1, 5, 20],
    diff_periods=[1, 5],
    rolling_functions=['mean', 'std', 'min', 'max']
)

# åˆå§‹åŒ–æ—¶åºç‰¹å¾æå–å™¨
extractor = TimeSeriesFeatures(config)

# æå–æ—¶åºç‰¹å¾
features = extractor.extract_features(time_series_data)
```

#### 4.2 åˆ¶åº¦è¯†åˆ«

```
from module_02_feature_engineering.temporal_features import RegimeFeatures, RegimeConfig

# é…ç½®åˆ¶åº¦è¯†åˆ«å‚æ•°
config = RegimeConfig(
    n_regimes=3,
    regime_method='hmm',  # 'hmm', 'gmm', 'kmeans', 'threshold'
    volatility_window=60
)

# åˆå§‹åŒ–åˆ¶åº¦ç‰¹å¾æå–å™¨
regime_extractor = RegimeFeatures(config)

# æ£€æµ‹å¸‚åœºåˆ¶åº¦
regime_states = regime_extractor.detect_market_regimes(data, features=['returns', 'volatility', 'volume'])
```

#### 4.3 å­£èŠ‚æ€§åˆ†æ

```
from module_02_feature_engineering.temporal_features import SeasonalityExtractor, SeasonalityConfig

# é…ç½®å­£èŠ‚æ€§åˆ†æ
config = SeasonalityConfig(
    method='stl',  # 'stl', 'classical', 'fourier', 'custom'
    seasonal_periods=[5, 7, 21, 63, 252]
)

# åˆå§‹åŒ–å­£èŠ‚æ€§æå–å™¨
seasonal_extractor = SeasonalityExtractor(config)

# æå–å­£èŠ‚æ€§ç‰¹å¾
decomposition = seasonal_extractor.extract_seasonality(time_series)
```

### 5. å›¾ç‰¹å¾åˆ†æ

#### 5.1 è‚¡ç¥¨å…³ç³»å›¾æ„å»º

```
from module_02_feature_engineering.graph_features import StockGraphBuilder, GraphConfig

# é…ç½®å›¾æ„å»ºå‚æ•°
config = GraphConfig(
    correlation_threshold=0.5,
    similarity_method='correlation',
    max_edges_per_node=20
)

# åˆå§‹åŒ–å›¾æ„å»ºå™¨
builder = StockGraphBuilder(config)

# æ„å»ºç›¸å…³æ€§å›¾
graph = builder.build_correlation_graph(price_data, return_data)

# æ„å»ºç›¸ä¼¼æ€§å›¾
graph = builder.build_similarity_graph(feature_data)
```

#### 5.2 ç¤¾åŒºæ£€æµ‹

```
from module_02_feature_engineering.graph_features import CommunityDetection, CommunityConfig

# é…ç½®ç¤¾åŒºæ£€æµ‹å‚æ•°
config = CommunityConfig(
    method='louvain',  # 'louvain', 'spectral', 'label_propagation'
    resolution=1.0,
    n_clusters=5
)

# åˆå§‹åŒ–ç¤¾åŒºæ£€æµ‹å™¨
detector = CommunityDetection(config)

# æ£€æµ‹ç¤¾åŒº
communities = detector.detect_communities(graph)
```

#### 5.3 å›¾åˆ†æå™¨

```
from module_02_feature_engineering.graph_features import GraphAnalyzer

# åˆå§‹åŒ–å›¾åˆ†æå™¨
analyzer = GraphAnalyzer()

# åˆ†æå›¾ç‰¹å¾
features = analyzer.analyze_graph_features(data)
```

### 6. æ·±åº¦å­¦ä¹ ç‰¹å¾

```
from module_02_feature_engineering.feature_extraction import DeepFeatures, DeepFeaturesConfig

# é…ç½®æ·±åº¦ç‰¹å¾å‚æ•°
config = DeepFeaturesConfig(
    encoding_dim=32,
    hidden_layers=[128, 64],
    dropout_rate=0.2,
    epochs=100
)

# åˆå§‹åŒ–æ·±åº¦ç‰¹å¾æå–å™¨
extractor = DeepFeatures(config)

# æå–æ·±åº¦ç‰¹å¾
features = extractor.extract_features(data)
```

### 7. ç»Ÿè®¡ç‰¹å¾

```
from module_02_feature_engineering.feature_extraction import StatisticalFeatures, StatisticalFeatureConfig

# é…ç½®ç»Ÿè®¡ç‰¹å¾å‚æ•°
config = StatisticalFeatureConfig(
    window_sizes=[20, 60, 252],
    quantiles=[0.25, 0.5, 0.75],
    calculate_moments=True
)

# åˆå§‹åŒ–ç»Ÿè®¡ç‰¹å¾æå–å™¨
extractor = StatisticalFeatures(config)

# æå–ç»Ÿè®¡ç‰¹å¾
features = extractor.extract_features(data)
```

## æ•°æ®åº“å’Œå­˜å‚¨ç®¡ç†

### 1. ç‰¹å¾æ•°æ®åº“ç®¡ç†

```
from module_02_feature_engineering import get_feature_database_manager

# è·å–æ•°æ®åº“ç®¡ç†å™¨
db_manager = get_feature_database_manager()

# ä¿å­˜æŠ€æœ¯æŒ‡æ ‡
db_manager.save_technical_indicators(symbol, indicators_data)

# ä¿å­˜å› å­æ•°æ®
db_manager.save_factor_data(factor_id, symbol, factor_values, factor_type)

# ä¿å­˜ç¥ç»å› å­
db_manager.save_neural_factor(factor_name, config, performance_metrics)

# ä¿å­˜å›¾ç‰¹å¾
db_manager.save_graph_features(analysis_date, graph_metrics)

# æŸ¥è¯¢æ•°æ®
indicators = db_manager.get_technical_indicators(symbol, start_date, end_date)
factors = db_manager.get_factor_data(factor_id, symbol, start_date, end_date)
```

### 2. ç¼“å­˜ç®¡ç†

```
from module_02_feature_engineering import FeatureCacheManager

# åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
cache = FeatureCacheManager(max_size=1000, ttl=3600)

# å­˜å‚¨æ•°æ®
cache.set(feature_type, symbol, data)

# è·å–æ•°æ®
cached_data = cache.get(feature_type, symbol)

# æ¸…ç†ç¼“å­˜
cache.clear()
```

## ä¾¿æ·å‡½æ•°

æ¨¡å—æä¾›äº†å¤šä¸ªä¾¿æ·å‡½æ•°ä»¥ç®€åŒ–å¸¸ç”¨æ“ä½œï¼š

```
# æŠ€æœ¯æŒ‡æ ‡è®¡ç®—
from module_02_feature_engineering import calculate_technical_indicators
indicators = calculate_technical_indicators(data)

# å› å­è¯„ä¼°
from module_02_feature_engineering.factor_discovery import evaluate_factor
result = evaluate_factor(factor_values, returns, "factor_name", config)

# é—ä¼ å› å­æœç´¢
from module_02_feature_engineering.factor_discovery import genetic_factor_search
best_gene, fitness = genetic_factor_search(data, target, config)

# å¸‚åœºåˆ¶åº¦æ£€æµ‹
from module_02_feature_engineering.temporal_features import detect_market_regimes
regimes = detect_market_regimes(data, n_regimes=3, method='gmm')

# å­£èŠ‚æ€§æå–
from module_02_feature_engineering.temporal_features import extract_seasonality
decomposition = extract_seasonality(time_series, method='stl')

# è‚¡ç¥¨å›¾æ„å»º
from module_02_feature_engineering.graph_features import build_stock_correlation_graph
graph = build_stock_correlation_graph(price_data, correlation_threshold=0.5)

# ç¤¾åŒºæ£€æµ‹
from module_02_feature_engineering.graph_features import detect_stock_communities
communities = detect_stock_communities(graph, method='louvain')
```

## æ•°æ®æµç¨‹

### å…¸å‹å·¥ä½œæµç¨‹

1. **æ•°æ®è·å–**ï¼šä» Module 01 è·å–æ¸…æ´—åçš„è‚¡ç¥¨æ•°æ®
2. **ç‰¹å¾è®¡ç®—**ï¼šä½¿ç”¨å„ç§ç‰¹å¾æå–å™¨è®¡ç®—ç‰¹å¾
3. **å› å­å‘ç°**ï¼šé€šè¿‡å¤šç§æ–¹æ³•å‘ç°æœ‰æ•ˆå› å­
4. **ç‰¹å¾å­˜å‚¨**ï¼šå°†ç»“æœä¿å­˜åˆ° Module 02 ä¸“ç”¨æ•°æ®åº“
5. **ç‰¹å¾æŸ¥è¯¢**ï¼šä¸ºåç»­æ¨¡å—æä¾›ç‰¹å¾æ•°æ®

### å®Œæ•´ç¤ºä¾‹

```
from module_02_feature_engineering import (
    FeatureEngineeringPipeline,
    TechnicalIndicators,
    FactorAnalyzer,
    get_feature_database_manager
)

# 1. åˆå§‹åŒ–ç»„ä»¶
pipeline = FeatureEngineeringPipeline()
calculator = TechnicalIndicators()
analyzer = FactorAnalyzer()
db_manager = get_feature_database_manager()

# 2. å¤„ç†æ•°æ®
symbol = "000001"
stock_data = load_data_from_module01(symbol)  # å‡è®¾çš„æ•°æ®åŠ è½½å‡½æ•°

# 3. è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
indicators = calculator.calculate_all_indicators(stock_data)

# 4. å› å­åˆ†æ
returns = stock_data['close'].pct_change().dropna()
rsi_factor = calculator.calculate_rsi(stock_data['close'])
factor_result = analyzer.analyze_factor(rsi_factor, returns)

# 5. ä¿å­˜ç»“æœ
db_manager.save_technical_indicators(symbol, indicators)
db_manager.save_factor_data(f"rsi_{symbol}", symbol, rsi_factor, "technical")

print(f"ç‰¹å¾å·¥ç¨‹å®Œæˆï¼ŒIC: {factor_result.ic:.4f}")
```

## é…ç½®å’Œè‡ªå®šä¹‰

### ç¯å¢ƒé…ç½®

æ¨¡å—ä¼šè‡ªåŠ¨åˆ›å»ºä¸“ç”¨çš„SQLiteæ•°æ®åº“æ–‡ä»¶ï¼š
- æ•°æ®åº“ä½ç½®ï¼š`module02_features.db`
- è‡ªåŠ¨åˆ›å»ºå¿…è¦çš„è¡¨ç»“æ„
- æ”¯æŒæ•°æ®ç‰ˆæœ¬ç®¡ç†å’Œæ¸…ç†

### æ‰©å±•å¼€å‘

æ¨¡å—é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒè‡ªå®šä¹‰æ‰©å±•ï¼š

1. **è‡ªå®šä¹‰æŠ€æœ¯æŒ‡æ ‡**ï¼šç»§æ‰¿ `TechnicalIndicators` ç±»
2. **è‡ªå®šä¹‰å› å­**ï¼šå®ç° `FactorAnalyzer` æ¥å£
3. **è‡ªå®šä¹‰ç‰¹å¾æå–å™¨**ï¼šç»§æ‰¿ç›¸åº”çš„åŸºç±»
4. **è‡ªå®šä¹‰å­˜å‚¨åç«¯**ï¼šå®ç°å­˜å‚¨æ¥å£

## æ€§èƒ½ä¼˜åŒ–

- **ç¼“å­˜æœºåˆ¶**ï¼šè‡ªåŠ¨ç¼“å­˜è®¡ç®—ç»“æœï¼Œé¿å…é‡å¤è®¡ç®—
- **æ‰¹é‡å¤„ç†**ï¼šæ”¯æŒæ‰¹é‡è®¡ç®—å¤šä¸ªè‚¡ç¥¨çš„ç‰¹å¾
- **å¢é‡æ›´æ–°**ï¼šæ”¯æŒå¢é‡æ›´æ–°å·²å­˜åœ¨çš„ç‰¹å¾æ•°æ®
- **å¹¶è¡Œè®¡ç®—**ï¼šéƒ¨åˆ†ç»„ä»¶æ”¯æŒå¤šè¿›ç¨‹å¹¶è¡Œè®¡ç®—

## æ³¨æ„äº‹é¡¹

1. **æ•°æ®ä¾èµ–**ï¼šç¡®ä¿ Module 01 æ•°æ®å¯ç”¨
2. **å†…å­˜ç®¡ç†**ï¼šå¤§é‡æ•°æ®å¤„ç†æ—¶æ³¨æ„å†…å­˜ä½¿ç”¨
3. **å‚æ•°è°ƒä¼˜**ï¼šæ ¹æ®å…·ä½“éœ€æ±‚è°ƒæ•´ç®—æ³•å‚æ•°
4. **æ•°æ®è´¨é‡**ï¼šè¾“å…¥æ•°æ®è´¨é‡ç›´æ¥å½±å“ç‰¹å¾è´¨é‡
5. **ç‰ˆæœ¬å…¼å®¹**ï¼šç¡®ä¿ä¸å…¶ä»–æ¨¡å—çš„ç‰ˆæœ¬å…¼å®¹æ€§

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **å¯¼å…¥é”™è¯¯**ï¼šæ£€æŸ¥ä¾èµ–åŒ…æ˜¯å¦æ­£ç¡®å®‰è£…
2. **æ•°æ®åº“é”™è¯¯**ï¼šç¡®è®¤æ•°æ®åº“æ–‡ä»¶æƒé™å’Œç£ç›˜ç©ºé—´
3. **è®¡ç®—é”™è¯¯**ï¼šæ£€æŸ¥è¾“å…¥æ•°æ®æ ¼å¼å’Œå®Œæ•´æ€§
4. **æ€§èƒ½é—®é¢˜**ï¼šè€ƒè™‘ä½¿ç”¨ç¼“å­˜å’Œè°ƒæ•´æ‰¹æ¬¡å¤§å°

### æ—¥å¿—å’Œè°ƒè¯•

æ¨¡å—é›†æˆäº†å®Œæ•´çš„æ—¥å¿—ç³»ç»Ÿï¼Œå¯é€šè¿‡ä»¥ä¸‹æ–¹å¼è·å–è¯¦ç»†ä¿¡æ¯ï¼š

```
import logging
logging.basicConfig(level=logging.DEBUG)
```

---

**æ¨¡å—ç‰ˆæœ¬**ï¼š2.0.0  
**æœ€åæ›´æ–°**ï¼š2024å¹´10æœˆ  
**ç»´æŠ¤å›¢é˜Ÿ**ï¼šFinLoomå¼€å‘å›¢é˜Ÿ
